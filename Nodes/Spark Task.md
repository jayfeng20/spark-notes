#core-concept
Each stage is comprised of Spark tasks (a unit of execution), which are then federated across each Spark executor; each task maps to a single core and works on a single partition of data (Figure 2-5). As such, an executor with 16 cores can have 16 or more tasks working on 16 or more partitions in parallel, making the execution of Sparkâ€™s tasks exceedingly parallel!![[Screenshot 2025-05-04 at 11.21.18 AM.png]]